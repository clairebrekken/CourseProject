{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering\n",
    "Create features to be used in conventional models. Features aim to\n",
    "capture characteristics of sarcastic tweets so that models are able\n",
    "to accurately classify tweets.\n",
    "\n",
    "Several functions in this notebook where inspired by the work in\n",
    "this repo: https://github.com/MirunaPislar/Sarcasm-Detection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jx_dev.utils_text_clf import utils_text_clf as utils\n",
    "import cb_dev.vocab_helpers as helper\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply RegEx to split terms into multiple words. Helper for hashtag features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def camel_case_split(term):\n",
    "    term = re.sub(r'([0-9]+)', r' \\1', term)\n",
    "    term = re.sub(r'(1st|2nd|3rd|4th|5th|6th|7th|8th|9th|0th)', r'\\1 ', term)\n",
    "    splits = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', term)\n",
    "    return [s.group(0) for s in splits]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split hashtags into individual terms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_hashtags(hashtag, word_list, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Hashtag is %s\" % hashtag)\n",
    "    # Get rid of the hashtag\n",
    "    if hashtag.startswith('#'):\n",
    "        term = hashtag[1:]\n",
    "    else:\n",
    "        term = hashtag\n",
    "\n",
    "    # If the hastag is already an existing word (a single word), return it\n",
    "    if word_list is not None and term.lower() in word_list:\n",
    "        return ['#' + term]\n",
    "    # First, attempt splitting by CamelCase\n",
    "    if term[1:] != term[1:].lower() and term[1:] != term[1:].upper():\n",
    "        splits = camel_case_split(term)\n",
    "    elif '#' in term:\n",
    "        splits = term.split(\"#\")\n",
    "    elif len(term) > 27:\n",
    "        if verbose:\n",
    "            print(\"Hashtag %s is too big so let as it is.\" % term)\n",
    "        splits = [term]\n",
    "    else:\n",
    "        # Second, build possible splits and choose the best split by assigning\n",
    "        # a \"score\" to each possible split, based on the frequency with which a word is occurring\n",
    "        penalty = -69971\n",
    "        max_coverage = penalty\n",
    "        max_splits = 6\n",
    "        n_splits = 0\n",
    "        term = re.sub(r'([0-9]+)', r' \\1', term)\n",
    "        term = re.sub(r'(1st|2nd|3rd|4th|5th|6th|7th|8th|9th|0th)', r'\\1 ', term)\n",
    "        term = re.sub(r'([A-Z][^A-Z ]+)', r' \\1', term.strip())\n",
    "        term = re.sub(r'([A-Z]{2,})+', r' \\1', term)\n",
    "        splits = term.strip().split(' ')\n",
    "        if len(splits) < 3:\n",
    "            # Splitting lower case and uppercase hashtags in up to 5 words\n",
    "            chars = [c for c in term.lower()]\n",
    "            found_all_words = False\n",
    "\n",
    "            while n_splits < max_splits and not found_all_words:\n",
    "                for index in itertools.combinations(range(0, len(chars)), n_splits):\n",
    "                    output = np.split(chars, index)\n",
    "                    line = [''.join(o) for o in output]\n",
    "                    score = 0.0\n",
    "                    for word in line:\n",
    "                        stripped = word.strip()\n",
    "                        if stripped in word_list:\n",
    "                            score += int(word_list.get(stripped))\n",
    "                        else:\n",
    "                            if stripped.isnumeric():  # not stripped.isalpha():\n",
    "                                score += 0.0\n",
    "                            else:\n",
    "                                score += penalty\n",
    "                    score = score / float(len(line))\n",
    "                    if score > max_coverage:\n",
    "                        splits = line\n",
    "                        max_coverage = score\n",
    "                        line_is_valid_word = [word.strip() in word_list if not word.isnumeric()\n",
    "                                              else True for word in line]\n",
    "                        if all(line_is_valid_word):\n",
    "                            found_all_words = True\n",
    "                n_splits = n_splits + 1\n",
    "    splits = ['#' + str(s) for s in splits]\n",
    "    if verbose:\n",
    "        print(\"Split to: \", splits)\n",
    "    return splits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial tweet cleaning - useful to filter data before tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, word_list, split_hashtag_method=split_hashtags, replace_user_mentions=True,\n",
    "                remove_hashtags=False, remove_emojis=False, all_to_lower_case=False):\n",
    "    # Add white space before every punctuation sign so that we can split around it and keep it\n",
    "    tweet = re.sub('([!?*&%\"~`^+{}])', r' \\1 ', tweet)\n",
    "    tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "    tokens = tweet.split()\n",
    "    valid_tokens = []\n",
    "    for word in tokens:\n",
    "        # Never include #sarca* hashtags\n",
    "        if word.lower().startswith('#sarca'):\n",
    "            continue\n",
    "        # Never include URLs\n",
    "        if 'http' in word:\n",
    "            continue\n",
    "        # Replace specific user mentions with a general user name\n",
    "        if replace_user_mentions and word.startswith('@'):\n",
    "            word = '@user'\n",
    "        # Split or remove hashtags\n",
    "        if word.startswith('#'):\n",
    "            if remove_hashtags:\n",
    "                continue\n",
    "            splits = split_hashtag_method(word[1:], word_list)\n",
    "            if all_to_lower_case:\n",
    "                valid_tokens.extend([split.lower() for split in splits])\n",
    "            else:\n",
    "                valid_tokens.extend(splits)\n",
    "            continue\n",
    "        if remove_emojis and word in emoji.UNICODE_EMOJI:\n",
    "            continue\n",
    "        if all_to_lower_case:\n",
    "            word = word.lower()\n",
    "        valid_tokens.append(word)\n",
    "    return ' '.join(valid_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper function for creating ngram features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_ngram_list(tokens, n):\n",
    "    tokens = [t for t in tokens if not t.startswith('#')]\n",
    "    tokens = [t for t in tokens if not t.startswith('@')]\n",
    "    ngram_list = [gram for gram in ngrams(tokens, n)]\n",
    "    return ngram_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper function for creating ngram features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_ngram_features(ngrams, ngram_map):\n",
    "    feature_list = [0] * np.zeros(len(ngram_map))\n",
    "    for gram in ngrams:\n",
    "        if gram in ngram_map:\n",
    "            feature_list[ngram_map[gram]] += 1.0\n",
    "    return feature_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper function for creating ngram features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_ngrams(tokens, n, stopwords):\n",
    "    if len(n) < 1:\n",
    "        return {}\n",
    "    filtered = []\n",
    "    for t in tokens:\n",
    "        if t not in stopwords and t.isalnum():\n",
    "            filtered.append(t)\n",
    "    tokens = filtered\n",
    "    ngram_features = {}\n",
    "    for i in n:\n",
    "        ngram_list = [gram for gram in ngrams(tokens, i)]\n",
    "        ngram_features[i] = ngram_list\n",
    "    return np.array(ngram_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper function for creating ngram features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_ngram_columns(df):\n",
    "    df[\"unigrams\"] = df[\"tokens\"].apply(get_ngram_list, args=[1])\n",
    "    df[\"bigrams\"] = df[\"tokens\"].apply(get_ngram_list, args=[2])\n",
    "    df[\"trigrams\"] = df[\"tokens\"].apply(get_ngram_list, args=[3])\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper function for creating ngram features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def final_ngram_cols(df, ngram_map):\n",
    "    df[\"unigram_features\"] = df[\"unigrams\"].apply(get_ngram_features, args=[ngram_map])\n",
    "    df[\"bigram_features\"] = df[\"bigrams\"].apply(get_ngram_features, args=[ngram_map])\n",
    "    df[\"trigram_features\"] = df[\"trigrams\"].apply(get_ngram_features, args=[ngram_map])\n",
    "    df[\"ngram_features\"] = df[\"unigram_features\"] + df[\"bigram_features\"] + df[\"trigram_features\"]\n",
    "\n",
    "    ngram_cols = [\"n-gram-\" + str(i) for i in range(len(ngram_map))]\n",
    "    df[ngram_cols] = pd.DataFrame(df.ngram_features.tolist(), index=df.index)\n",
    "    df[\"ngram_feature\"] = df[ngram_cols].sum(axis=1)\n",
    "    df.drop(columns=ngram_cols, inplace=True)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create features based on tweet characteristics such as number of users tagged"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_simple_features(df, emoji_sent_dict):\n",
    "    tokenizer = TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=True)\n",
    "    df[\"users_tagged\"] = df.response.str.count(\"@USER\")\n",
    "    df[\"response\"] = df.response.str.replace('@USER', '').str.strip()\n",
    "    df[\"tokens\"] = df[\"response\"].str.split()\n",
    "\n",
    "    df[\"num_hashtags\"] = df.response.str.count(\"#\")\n",
    "    df[\"num_capital\"] = df.response.str.findall(r'[A-Z]').str.len()\n",
    "    # Currently includes punctuation\n",
    "    df[\"tweet_length_words\"] = df.tokens.str.len()\n",
    "    df[\"tweet_length_char\"] = df.tokens.apply(lambda tweet: sum([len(w) for w in tweet]))\n",
    "    df[\"average_token_length\"] = df[\"tweet_length_words\"].astype(float) / df[\"tweet_length_char\"]\n",
    "\n",
    "    df[\"contains_laughter\"] = df.tokens.apply(\n",
    "        lambda tweet: len([1 for w in tweet if (w.lower().startswith(\"haha\")) or (re.match('l(o)+l$', w.lower()))])\n",
    "    )\n",
    "    df[\"contains_ellipses\"] = df.tokens.apply(lambda tweet: len([w for w in tweet if (w == '...') | (w == '..')]))\n",
    "    df[\"strong_negations\"] = df.tokens.apply(lambda tweet: len([w for w in tweet if w in helper.strong_negations]))\n",
    "    df[\"strong_affirmatives\"] = df.tokens.apply(lambda tweet: len([w for w in tweet if w in helper.strong_affirmatives]))\n",
    "    df[\"interjections\"] = df.tokens.apply(lambda tweet: len([w for w in tweet if w in helper.interjections]))\n",
    "    df[\"intensifiers\"] = df.tokens.apply(lambda tweet: len([w for w in tweet if w in helper.intensifiers]))\n",
    "    df[\"punctuation\"] = df.tokens.apply(lambda tweet: len([w for w in tweet if w in helper.punctuation]))\n",
    "    df[\"emojis\"] = df.tokens.apply(lambda tweet: len([w for w in tweet if w in emoji.UNICODE_EMOJI]))\n",
    "    df['negative emoji'] = df.tokens.apply(lambda tweet: sum([float(emoji_sent_dict[t][0]) for t in tweet if t in emoji_sent_dict.keys()]))\n",
    "    df['neutral emoji'] = df.tokens.apply(lambda tweet: sum([float(emoji_sent_dict[t][1]) for t in tweet if t in emoji_sent_dict.keys()]))\n",
    "    df['positive emoji'] = df.tokens.apply(lambda tweet: sum([float(emoji_sent_dict[t][2]) for t in tweet if t in emoji_sent_dict.keys()]))\n",
    "\n",
    "    # Clean up responses for tokenizing\n",
    "    df[\"tokens\"] = df[\"response\"].apply(tokenizer.tokenize)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply similar logic to each tweet in the context of a response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def context_features(df, emoji_sent_dict):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    users_tagged = 0\n",
    "    tokens = []\n",
    "\n",
    "    num_hashtags = 0\n",
    "    num_capital = 0\n",
    "    # Currently includes punctuation\n",
    "    tweet_length_words = 0\n",
    "    tweet_length_char = 0\n",
    "    average_token_length = 0\n",
    "\n",
    "    contains_laughter = 0\n",
    "    contains_ellipses = 0\n",
    "    strong_negations = 0\n",
    "    strong_affirmatives = 0\n",
    "    interjections = 0\n",
    "    intensifiers = 0\n",
    "    punctuation = 0\n",
    "    emojis = 0\n",
    "    negative_emoji = 0\n",
    "    neutral_emoji = 0\n",
    "    positive_emoji = 0\n",
    "\n",
    "    for tweet in df.loc['context']:\n",
    "        users_tagged += tweet.count(\"@USER\")\n",
    "        tokens.append(tokenizer.tokenize(tweet))\n",
    "\n",
    "        num_hashtags += tweet.count(\"#\")\n",
    "        num_capital += tweet.count(r'[A-Z]')\n",
    "        # Currently includes punctuation\n",
    "        tweet_length_words += len(tweet.split())\n",
    "        tweet_length_char += len(tweet)\n",
    "        average_token_length += len(tweet.split())/len(tweet)\n",
    "        for w in tweet:\n",
    "            if (w.lower().startswith(\"haha\")) or (re.match('l(o)+l$', w.lower())):\n",
    "                contains_laughter += 1\n",
    "\n",
    "        contains_ellipses += len([w for w in tweet if (w == '...') | (w == '..')])\n",
    "        strong_negations += len([w for w in tweet if w in helper.strong_negations])\n",
    "        strong_affirmatives += len([w for w in tweet if w in helper.strong_affirmatives])\n",
    "        interjections += len([w for w in tweet if w in helper.interjections])\n",
    "        intensifiers += len([w for w in tweet if w in helper.intensifiers])\n",
    "        punctuation += len([w for w in tweet if w in helper.punctuation])\n",
    "        emojis += len([w for w in tweet if w in emoji.UNICODE_EMOJI])\n",
    "        negative_emoji += sum([float(emoji_sent_dict[t][0]) for t in tweet if t in emoji_sent_dict.keys()])\n",
    "        neutral_emoji += sum([float(emoji_sent_dict[t][1]) for t in tweet if t in emoji_sent_dict.keys()])\n",
    "        positive_emoji += sum([float(emoji_sent_dict[t][2]) for t in tweet if t in emoji_sent_dict.keys()])\n",
    "    df[\"context_users_tagged\"] = users_tagged\n",
    "    df[\"context_tokens\"] = tokens\n",
    "    df[\"context_num_hashtags\"] = num_hashtags\n",
    "    df[\"context_num_capital\"] = num_capital\n",
    "    df[\"context_tweet_length_words\"] = tweet_length_words\n",
    "    df[\"context_tweet_length_char\"] = tweet_length_char\n",
    "    df[\"context_average_token_length\"] = average_token_length\n",
    "    df[\"context_contains_laughter\"] = contains_laughter\n",
    "    df[\"context_contains_ellipses\"] = contains_ellipses\n",
    "    df[\"context_strong_negations\"] = strong_negations\n",
    "    df[\"context_strong_affirmatives\"] = strong_affirmatives\n",
    "    df[\"context_interjections\"] = interjections\n",
    "    df[\"context_intensifiers\"] = intensifiers\n",
    "    df[\"context_punctuation\"] = punctuation\n",
    "    df[\"context_emojis\"] = emojis\n",
    "    df['context_negative emoji'] = negative_emoji\n",
    "    df['context_neutral emoji'] = neutral_emoji\n",
    "    df['context_positive emoji'] = positive_emoji\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load list of stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopword_list = []\n",
    "\n",
    "with open(\"stopwords.txt\", 'r') as file:\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    stopword_list = text.split(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load dictionary of words with associated popularity values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_list = {}\n",
    "\n",
    "with open(\"word_list.txt\", 'r') as file:\n",
    "    lines = file.read()\n",
    "    file.close()\n",
    "    for line in lines.split(\"\\n\"):\n",
    "        key, value = line.split(\"\\t\")\n",
    "        word_list[key] = value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load list of emojis with sentiment information"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emoji_sent_dict = {}\n",
    "with open(\"emoji_sent_dict.txt\", 'r') as file:\n",
    "    lines = file.read()\n",
    "    file.close()\n",
    "    for line in lines.split(\"\\n\"):\n",
    "        key = line.split(\"\\t\")[0]\n",
    "        value = line.split(\"\\t\")[1:]\n",
    "        emoji_sent_dict[key] = value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load train data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(os.getcwd()) + '/data'\n",
    "filename = 'train.jsonl'\n",
    "file = os.path.join(data_dir, filename)  # .json file\n",
    "train_df = utils.parse_json(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_filename = 'test.jsonl'\n",
    "test_file = os.path.join(data_dir, test_filename)  # .json file\n",
    "test_df = utils.parse_json(test_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df[\"response\"] = train_df.response.apply(clean_tweet, args=[word_list])\n",
    "test_df[\"response\"] = test_df.response.apply(clean_tweet, args=[word_list])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get simple featues for response tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = get_simple_features(train_df, emoji_sent_dict)\n",
    "test_df = get_simple_features(test_df, emoji_sent_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a column for each unigram, bigram and trigram that occurs in the train data. Same number of time each\n",
    "n-gram occurs in the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "train_df = create_ngram_columns(train_df)\n",
    "test_df = create_ngram_columns(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Update counters for each n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df[\"unigrams\"].apply(unigrams.update)\n",
    "train_df[\"bigrams\"].apply(bigrams.update)\n",
    "train_df[\"trigrams\"].apply(trigrams.update)\n",
    "print(train_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep n-grams that occur more than once in data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigram_tokens = [k for k, c in unigrams.items() if c > 1]\n",
    "bigram_tokens = [k for k, c in bigrams.items() if c > 1]\n",
    "trigram_tokens = [k for k, c in trigrams.items() if c > 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dictionary of n-grams that occur more than once"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ngram_map = dict()\n",
    "all_ngrams = unigram_tokens\n",
    "all_ngrams.extend(bigram_tokens)\n",
    "all_ngrams.extend(trigram_tokens)\n",
    "for i in range(0, len(all_ngrams)):\n",
    "    ngram_map[all_ngrams[i]] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep n-gram columns for n-grams that occur most often then add all n-grams columns together for one feature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = final_ngram_cols(train_df, ngram_map)\n",
    "test_df = final_ngram_cols(test_df, ngram_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get context features for each tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = train_df.apply(context_features, axis=1, args=[emoji_sent_dict])\n",
    "test_df = test_df.apply(context_features, axis=1, args=[emoji_sent_dict])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop unneeded columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"context\",\n",
    "    \"tokens\",\n",
    "    \"response\",\n",
    "    \"unigrams\",\n",
    "    \"bigrams\",\n",
    "    \"trigrams\",\n",
    "    \"unigram_features\",\n",
    "    \"bigram_features\",\n",
    "    \"trigram_features\",\n",
    "    \"ngram_features\",\n",
    "    \"context_tokens\"\n",
    "]\n",
    "train_df.drop(columns=cols_to_drop, inplace=True)\n",
    "test_df.drop(columns=cols_to_drop, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_df.dtypes)\n",
    "print(test_df.dtypes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.fillna(0, inplace=True)\n",
    "test_df.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_df.columns)\n",
    "print(test_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.to_csv(\"data/train_feature_engineering.csv\", index=False)\n",
    "test_df.to_csv(\"data/test_feature_engineering.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}